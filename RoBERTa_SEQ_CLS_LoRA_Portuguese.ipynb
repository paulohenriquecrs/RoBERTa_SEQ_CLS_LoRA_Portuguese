{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install modules\n",
    "Note: You also might need to enter an API key from [Weights & Biases](https://wandb.ai/login). In that case, you should import the library with `import wandb` and then run `wandb.login(key=\"your_api_key_here\")`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (4.48.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: evaluate in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: peft in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: psutil in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: networkx in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\paulo couto\\anaconda3\\envs\\mva\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate accelerate peft Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paulo Couto\\anaconda3\\envs\\mva\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForSequenceClassification\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and dataset configurations\n",
    "peft_model_name = 'roberta-portuguese-peft'\n",
    "modified_base = 'roberta-portuguese-modified'\n",
    "base_model = 'roberta-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training params\n",
    "n_epochs = 3\n",
    "batch_size = 4\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# LoRA params\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset in Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since LIACC/Emakhuwa-Portuguese-News-MT couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\Paulo Couto\\.cache\\huggingface\\datasets\\LIACC___emakhuwa-portuguese-news-mt\\default\\0.0.0\\f9c6df57e9d419503712979dd3272e2bfc15bbfc (last modified on Tue Feb 18 20:27:46 2025).\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('LIACC/Emakhuwa-Portuguese-News-MT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of instance in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seg_index': 2,\n",
       " 'pt': 'Matias Guente, do Canal de Mo√ßambique, vence Pr√©mio Internacional de Liberdade de Imprensa',\n",
       " 'vmw': 'Matias Guene, ooKanaale ya Mocampiikhi, oolola e peremiyo internasionaale ya woopowa wa imprensa',\n",
       " 'source': 'a_matias-guente-do-canal-de-mocÃßambique-vence-preÃÅmio-internacional-de-liberdade-de-imprensa_5930109.txt',\n",
       " 'project_title': 'desporto-cultura-1',\n",
       " 'category': 'cultura',\n",
       " 'domain': 'news',\n",
       " 'writting_style': 'standard',\n",
       " 'job_id': '6121524-e1d3e4d73a0f',\n",
       " 'translators': 'Raja,benedito',\n",
       " 'project_id': 6121524.0,\n",
       " 'segment_id': 2591966325.0,\n",
       " 'i_segment_id': 612152459301090.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll use the `\"pt\"` column as input and `\"category\"` as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Matias Guente, do Canal de Mo√ßambique, vence Pr√©mio Internacional de Liberdade de Imprensa\n",
      "Label: cultura\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input text: {dataset['train'][0]['pt']}\")\n",
    "print(f\"Label: {dataset['train'][0]['category']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all sets (train, test, validation) and filter by labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define selected labels\n",
    "selected_labels = {'cultura', 'desporto', 'economia', 'mundo', 'saude'}\n",
    "num_labels = len(selected_labels)\n",
    "\n",
    "# Merge all dataset splits into a single list\n",
    "all_data = {\n",
    "    'pt': dataset['train']['pt'] + dataset['validation']['pt'] + dataset['test']['pt'],\n",
    "    'category': dataset['train']['category'] + dataset['validation']['category'] + dataset['test']['category']\n",
    "}\n",
    "\n",
    "# Filter dataset to only include selected labels\n",
    "filtered_data = {\n",
    "    'pt': [],\n",
    "    'category': [],\n",
    "    'labels': []\n",
    "}\n",
    "\n",
    "# Convert category names to numeric labels\n",
    "label2id = {label: i for i, label in enumerate(sorted(selected_labels))}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "for text, category in zip(all_data['pt'], all_data['category']):\n",
    "    if category in selected_labels:\n",
    "        filtered_data['pt'].append(text)\n",
    "        filtered_data['category'].append(category)\n",
    "        filtered_data['labels'].append(label2id[category])\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "full_dataset = Dataset.from_dict(filtered_data)\n",
    "\n",
    "# Shuffle dataset\n",
    "full_dataset = full_dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-split the dataset into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset (80% train, 10% validation, 10% test)\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "\n",
    "train_test_split = full_dataset.train_test_split(test_size=(val_size + test_size), seed=42)\n",
    "val_test_split = train_test_split['test'].train_test_split(test_size=(test_size / (val_size + test_size)), seed=42)\n",
    "\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = val_test_split['train']\n",
    "test_dataset = val_test_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11478/11478 [00:11<00:00, 1029.65 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1435/1435 [00:01<00:00, 906.76 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1435/1435 [00:01<00:00, 725.16 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset sizes:\n",
      "Train set: 11478\n",
      "Eval set: 1435\n",
      "Test set: 1435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenization function\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples['pt'], truncation=True, padding=True)\n",
    "\n",
    "# Tokenize the datasets and remove unnecessary columns\n",
    "train_dataset = train_dataset.map(preprocess, batched=True, remove_columns=['pt', 'category'])\n",
    "eval_dataset = eval_dataset.map(preprocess, batched=True, remove_columns=['pt', 'category'])\n",
    "test_dataset = test_dataset.map(preprocess, batched=True, remove_columns=['pt', 'category'])\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"\\nDataset sizes:\")\n",
    "print(f\"Train set: {len(train_dataset)}\")\n",
    "print(f\"Eval set: {len(eval_dataset)}\")\n",
    "print(f\"Test set: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paulo Couto\\anaconda3\\envs\\mva\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='steps',\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=n_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    ")\n",
    "\n",
    "def get_trainer(model):\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the base model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model\n",
      "trainable params: 889,349 || all params: 125,538,826 || trainable%: 0.7084\n"
     ]
    }
   ],
   "source": [
    "# Configure and create PEFT model\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False,\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "print('PEFT Model')\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8610' max='8610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8610/8610 9:20:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.474600</td>\n",
       "      <td>1.060661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.117800</td>\n",
       "      <td>0.961510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.081800</td>\n",
       "      <td>0.960028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.040800</td>\n",
       "      <td>0.962164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.047700</td>\n",
       "      <td>0.925508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.997700</td>\n",
       "      <td>0.915953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.985500</td>\n",
       "      <td>0.940330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.891503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.982700</td>\n",
       "      <td>0.876724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.948700</td>\n",
       "      <td>0.863613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.921200</td>\n",
       "      <td>0.861088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.917900</td>\n",
       "      <td>0.868663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.901800</td>\n",
       "      <td>0.870210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.924100</td>\n",
       "      <td>0.856808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.899600</td>\n",
       "      <td>0.855585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.930400</td>\n",
       "      <td>0.847937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.892500</td>\n",
       "      <td>0.850826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 06:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_lora_finetuning_trainer = get_trainer(peft_model)\n",
    "peft_lora_finetuning_trainer.train()\n",
    "peft_lora_finetuning_trainer.evaluate()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "tokenizer.save_pretrained(modified_base)\n",
    "peft_model.save_pretrained(peft_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference functions\n",
    "def load_model_for_inference():\n",
    "    inference_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "        peft_model_name,\n",
    "        id2label=id2label\n",
    "    )\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(modified_base)\n",
    "    return inference_model, tokenizer\n",
    "\n",
    "def classify(text, inference_model, tokenizer, gold):\n",
    "    device = next(inference_model.parameters()).device  # Get the device the model is on\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    # Move input tensors to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = inference_model(**inputs)\n",
    "    prediction = output.logits.argmax(dim=-1).item()\n",
    "    emoji = \"‚úÖ\" if id2label[prediction] == gold else \"‚ùå\"\n",
    "\n",
    "    print(f'Text: {text}\\nPrediction: {id2label[prediction]}\\nGold: {gold}\\n{emoji}\\n')\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(inference_model, dataset):\n",
    "    metric = evaluate.load('accuracy')\n",
    "    eval_dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inference_model.to(device)\n",
    "    inference_model.eval()\n",
    "    \n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "        metric.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=batch[\"labels\"]\n",
    "        )\n",
    "    \n",
    "    eval_metric = metric.compute()\n",
    "    return eval_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the latest cached version of the module from C:\\Users\\Paulo Couto\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--accuracy\\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Fri Jan  3 17:49:54 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 359/359 [07:44<00:00,  1.29s/it]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the latest cached version of the module from C:\\Users\\Paulo Couto\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--accuracy\\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Fri Jan  3 17:49:54 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 359/359 [08:04<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "original_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label\n",
    ")\n",
    "base_perf = evaluate_model(original_model, test_dataset)\n",
    "\n",
    "# Evaluate LoRA fine-tuned model\n",
    "inference_model, tokenizer = load_model_for_inference()\n",
    "lora_perf = evaluate_model(inference_model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare test accuracies: Base vs LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model performance: 0.190\n",
      "LoRA Fine-tuned model performance: 0.702\n"
     ]
    }
   ],
   "source": [
    "print(f\"Base model performance: {base_perf['accuracy']:.3f}\")\n",
    "print(f\"LoRA Fine-tuned model performance: {lora_perf['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on some dummy examples created by me (native speaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: O chanceler se encontrou ontem com o Primeiro Ministro brit√¢nico para discutir as rela√ß√µes comerciais entre os dois pa√≠ses\n",
      "Prediction: mundo\n",
      "Gold: mundo\n",
      "‚úÖ\n",
      "\n",
      "Text: O Banco Central decidiu aumentar novamente a taxa SELIC para tentar combater as altas da infla√ß√£o\n",
      "Prediction: mundo\n",
      "Gold: economia\n",
      "‚ùå\n",
      "\n",
      "Text: O Cruzeiro anunciou a contrata√ß√£o de um novo treinador para o restante da temporada de 2025\n",
      "Prediction: mundo\n",
      "Gold: desporto\n",
      "‚ùå\n",
      "\n",
      "Text: O filme Ainda Estou aqui, que conta com a atriz Fernanda Torres, foi indicado a tr√™s premios, incluindo Melhor Filme\n",
      "Prediction: mundo\n",
      "Gold: cultura\n",
      "‚ùå\n",
      "\n",
      "Text: J√µao Fonseca ganhou ontem seu primeiro torneio na Argentina\n",
      "Prediction: mundo\n",
      "Gold: desporto\n",
      "‚ùå\n",
      "\n",
      "Text: Aumenta o n√∫mero de casos de catapora e sarampo nas escolas da rede p√∫blica do pa√≠s\n",
      "Prediction: mundo\n",
      "Gold: saude\n",
      "‚ùå\n",
      "\n",
      "Text: O Ministro da Fazenda estuda implementar uma taxa sobre importa√ß√µes vindas da China\n",
      "Prediction: mundo\n",
      "Gold: economia\n",
      "‚ùå\n",
      "\n",
      "Text: Representantes de R√∫ssia e Estados Unidos se reuniram para debater o fim da guerra na Ucr√¢nina\n",
      "Prediction: mundo\n",
      "Gold: mundo\n",
      "‚úÖ\n",
      "\n",
      "Text: O surto de casos de dengue em S√£o Paulo fez com que o governo intesificasse a campanha de vacina√ß√£o\n",
      "Prediction: mundo\n",
      "Gold: saude\n",
      "‚ùå\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test classification\n",
    "sample_text = \"O chanceler se encontrou ontem com o Primeiro Ministro brit√¢nico para discutir as rela√ß√µes comerciais entre os dois pa√≠ses\"\n",
    "gold = \"mundo\"\n",
    "classify(sample_text, original_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"O Banco Central decidiu aumentar novamente a taxa SELIC para tentar combater as altas da infla√ß√£o\"\n",
    "gold = \"economia\"\n",
    "classify(sample_text, original_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"O Cruzeiro anunciou a contrata√ß√£o de um novo treinador para o restante da temporada de 2025\"\n",
    "gold = \"desporto\"\n",
    "classify(sample_text, original_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"O filme Ainda Estou aqui, que conta com a atriz Fernanda Torres, foi indicado a tr√™s premios, incluindo Melhor Filme\"\n",
    "gold = \"cultura\"\n",
    "classify(sample_text, original_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"J√µao Fonseca ganhou ontem seu primeiro torneio na Argentina\"\n",
    "gold = \"desporto\"\n",
    "classify(sample_text, original_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"Aumenta o n√∫mero de casos de catapora e sarampo nas escolas da rede p√∫blica do pa√≠s\"\n",
    "gold = \"saude\"\n",
    "classify(sample_text, original_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"O Ministro da Fazenda estuda implementar uma taxa sobre importa√ß√µes vindas da China\"\n",
    "gold = \"economia\"\n",
    "classify(sample_text, original_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"Representantes de R√∫ssia e Estados Unidos se reuniram para debater o fim da guerra na Ucr√¢nina\"\n",
    "gold = \"mundo\"\n",
    "classify(sample_text, original_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"O surto de casos de dengue em S√£o Paulo fez com que o governo intesificasse a campanha de vacina√ß√£o\"\n",
    "gold = \"saude\"\n",
    "classify(sample_text, original_model, tokenizer, gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: O chanceler se encontrou ontem com o Primeiro Ministro brit√¢nico para discutir as rela√ß√µes comerciais entre os dois pa√≠ses\n",
      "Prediction: mundo\n",
      "Gold: mundo\n",
      "‚úÖ\n",
      "\n",
      "Text: O Banco Central decidiu aumentar novamente a taxa SELIC para tentar combater as altas da infla√ß√£o\n",
      "Prediction: economia\n",
      "Gold: economia\n",
      "‚úÖ\n",
      "\n",
      "Text: O Cruzeiro anunciou a contrata√ß√£o de um novo treinador para o restante da temporada de 2025\n",
      "Prediction: desporto\n",
      "Gold: desporto\n",
      "‚úÖ\n",
      "\n",
      "Text: O filme Ainda Estou aqui, que conta com a atriz Fernanda Torres, foi indicado a tr√™s premios, incluindo Melhor Filme\n",
      "Prediction: cultura\n",
      "Gold: cultura\n",
      "‚úÖ\n",
      "\n",
      "Text: J√µao Fonseca ganhou ontem seu primeiro torneio na Argentina\n",
      "Prediction: desporto\n",
      "Gold: desporto\n",
      "‚úÖ\n",
      "\n",
      "Text: Aumenta o n√∫mero de casos de catapora e sarampo nas escolas da rede p√∫blica do pa√≠s\n",
      "Prediction: saude\n",
      "Gold: saude\n",
      "‚úÖ\n",
      "\n",
      "Text: O Ministro da Fazenda estuda implementar uma taxa sobre importa√ß√µes vindas da China\n",
      "Prediction: economia\n",
      "Gold: economia\n",
      "‚úÖ\n",
      "\n",
      "Text: Representantes de R√∫ssia e Estados Unidos se reuniram para debater o fim da guerra na Ucr√¢nina\n",
      "Prediction: mundo\n",
      "Gold: mundo\n",
      "‚úÖ\n",
      "\n",
      "Text: O surto de casos de dengue em S√£o Paulo fez com que o governo intesificasse a campanha de vacina√ß√£o\n",
      "Prediction: saude\n",
      "Gold: saude\n",
      "‚úÖ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test classification\n",
    "sample_text = \"O chanceler se encontrou ontem com o Primeiro Ministro brit√¢nico para discutir as rela√ß√µes comerciais entre os dois pa√≠ses\"\n",
    "gold = \"mundo\"\n",
    "classify(sample_text, inference_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"O Banco Central decidiu aumentar novamente a taxa SELIC para tentar combater as altas da infla√ß√£o\"\n",
    "gold = \"economia\"\n",
    "classify(sample_text, inference_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"O Cruzeiro anunciou a contrata√ß√£o de um novo treinador para o restante da temporada de 2025\"\n",
    "gold = \"desporto\"\n",
    "classify(sample_text, inference_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"O filme Ainda Estou aqui, que conta com a atriz Fernanda Torres, foi indicado a tr√™s premios, incluindo Melhor Filme\"\n",
    "gold = \"cultura\"\n",
    "classify(sample_text, inference_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"J√µao Fonseca ganhou ontem seu primeiro torneio na Argentina\"\n",
    "gold = \"desporto\"\n",
    "classify(sample_text, inference_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"Aumenta o n√∫mero de casos de catapora e sarampo nas escolas da rede p√∫blica do pa√≠s\"\n",
    "gold = \"saude\"\n",
    "classify(sample_text, inference_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"O Ministro da Fazenda estuda implementar uma taxa sobre importa√ß√µes vindas da China\"\n",
    "gold = \"economia\"\n",
    "classify(sample_text, inference_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"Representantes de R√∫ssia e Estados Unidos se reuniram para debater o fim da guerra na Ucr√¢nina\"\n",
    "gold = \"mundo\"\n",
    "classify(sample_text, inference_model, tokenizer, gold)\n",
    "\n",
    "sample_text = \"O surto de casos de dengue em S√£o Paulo fez com que o governo intesificasse a campanha de vacina√ß√£o\"\n",
    "gold = \"saude\"\n",
    "classify(sample_text, inference_model, tokenizer, gold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
